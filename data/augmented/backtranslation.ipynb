{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e29964e",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b0bd0",
   "metadata": {},
   "source": [
    "### Backtranslation with facebook/nllb-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fda171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f918e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/NLP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Backtranslation with facebook/nllb-200-distilled-600M\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model_name1 = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(model_name1)\n",
    "\n",
    "model_name2 = \"Helsinki-NLP/opus-mt-fr-es\"\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(model_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd256091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "translatorES2EN = pipeline('translation', model=model1, tokenizer=tokenizer1, src_lang='es_Latn', tgt_lang='fra_Latn', max_length=512)\n",
    "\n",
    "translatorEN2ES = pipeline('translation', model=model2, tokenizer=tokenizer2, src_lang='fra', tgt_lang='spa', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678a2b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/roicort/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def find_synonyms(text):\n",
    "    words = text.split()\n",
    "    augmented_text = []\n",
    "    for word in words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            augmented_text.append(synonym)\n",
    "        else:\n",
    "            augmented_text.append(word)\n",
    "    return ' '.join(augmented_text)\n",
    "\n",
    "def back_translate(text, trans1, trans2):\n",
    "    translated_text = trans1(text)[0]['translation_text']\n",
    "    #wordenetd_text = find_synonyms(translated_text)\n",
    "    #print('Translated:', translated_text)\n",
    "    #print('Wordnet:', wordenetd_text)\n",
    "    backtranslated = trans2(translated_text)[0]['translation_text']\n",
    "    return backtranslated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f80628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'/Users/roicort/GitHub/REST-MEX25/dataset/train.csv')\n",
    "\n",
    "data['Title'] = data['Title'].astype(str)\n",
    "data['Review'] = data['Review'].astype(str)\n",
    "data['Town'] = data['Town'].astype(str)\n",
    "data['Region'] = data['Region'].astype(str)\n",
    "data['Type'] = data['Type'].astype(str)\n",
    "data['Polarity'] = data['Polarity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37ad01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews1: 32646\n",
      "Reviews2: 32976\n",
      "Reviews3: 93114\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "\n",
    "Reviews1 = data[data['Polarity'] == 1]\n",
    "Reviews2 = data[data['Polarity'] == 2]\n",
    "Reviews3 = data[data['Polarity'] == 3]\n",
    "\n",
    "print('Reviews1:', Reviews1.size)\n",
    "print('Reviews2:', Reviews2.size)\n",
    "print('Reviews3:', Reviews3.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc3ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1430/1500 [1:52:40<04:20,  3.73s/it] Your input_length: 499 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "100%|██████████| 1500/1500 [1:58:22<00:00,  4.73s/it]\n",
      "  1%|▏         | 20/1500 [01:35<1:27:51,  3.56s/it]Your input_length: 501 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "100%|██████████| 1500/1500 [1:53:48<00:00,  4.55s/it] \n",
      "100%|██████████| 1500/1500 [1:46:46<00:00,  4.27s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0                                   ¡Es una canción!   \n",
      "1                             No hay recomendaciones   \n",
      "2               La comida es un asco, nos hemos ido.   \n",
      "3  Cuando los COBRAS están a la baja, la comida d...   \n",
      "4                                 La cena de Navidad   \n",
      "\n",
      "                                              Review  \\\n",
      "0  El lugar es increíble (como todo Tulúm), pero ...   \n",
      "1  La comida es bastante mala y tiene poca higien...   \n",
      "2  La comida es una deshonra, nos fuimos hoy el 2...   \n",
      "3  Cenamos ayer, la decoración del lugar es hermo...   \n",
      "4  Había mucho que desear el servicio, las entrad...   \n",
      "\n",
      "                         Town       Region        Type  Polarity  \n",
      "0                       Tulum  QuintanaRoo       Hotel         1  \n",
      "1  San_Cristobal_de_las_Casas      Chiapas  Restaurant         1  \n",
      "2                     Cholula       Puebla  Restaurant         1  \n",
      "3                 Tlaquepaque      Jalisco  Restaurant         1  \n",
      "4  San_Cristobal_de_las_Casas      Chiapas  Restaurant         1  \n",
      "Total rows: 4500\n",
      "Total rows: 208051\n"
     ]
    }
   ],
   "source": [
    "new_rows1 = []\n",
    "for i in tqdm(random.sample(range(0, len(Reviews1)), 1500)):\n",
    "    new_row = {\n",
    "        'Title':  back_translate(Reviews1['Title'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Review': back_translate(Reviews1['Review'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Town': Reviews1['Town'].iloc[i],\n",
    "        'Region': Reviews1['Region'].iloc[i],\n",
    "        'Type': Reviews1['Type'].iloc[i],\n",
    "        'Polarity': Reviews1['Polarity'].iloc[i]\n",
    "    }\n",
    "\n",
    "    new_rows1.append(new_row)\n",
    "\n",
    "new_rows2 = []\n",
    "for i in tqdm(random.sample(range(0, len(Reviews2)), 1500)):\n",
    "    new_row = {\n",
    "        'Title':  back_translate(Reviews2['Title'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Review': back_translate(Reviews2['Review'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Town': Reviews2['Town'].iloc[i],\n",
    "        'Region': Reviews2['Region'].iloc[i],\n",
    "        'Type': Reviews2['Type'].iloc[i],\n",
    "        'Polarity': Reviews2['Polarity'].iloc[i]\n",
    "    }\n",
    "\n",
    "    new_rows2.append(new_row)\n",
    "\n",
    "new_rows3 = []\n",
    "for i in tqdm(random.sample(range(0, len(Reviews3)), 1500)):\n",
    "    new_row = {\n",
    "        'Title':  back_translate(Reviews3['Title'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Review': back_translate(Reviews3['Review'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Town': Reviews3['Town'].iloc[i],\n",
    "        'Region': Reviews3['Region'].iloc[i],\n",
    "        'Type': Reviews3['Type'].iloc[i],\n",
    "        'Polarity': Reviews3['Polarity'].iloc[i]\n",
    "    }\n",
    "\n",
    "    new_rows3.append(new_row)\n",
    "\n",
    "# Crear un nuevo DataFrame con las nuevas filas\n",
    "new_data1 = pd.DataFrame(new_rows1)\n",
    "new_data2 = pd.DataFrame(new_rows2)\n",
    "new_data3 = pd.DataFrame(new_rows3)\n",
    "# Concatenar los DataFrames\n",
    "new_data = pd.concat([new_data1, new_data2, new_data3], ignore_index=True)\n",
    "# Guardar el nuevo DataFrame en un archivo CSV\n",
    "new_data.to_csv(r'/Users/roicort/GitHub/REST-MEX25/dataset/train_backtranslated.csv', index=False)\n",
    "\n",
    "# Imprimir el nuevo DataFrame\n",
    "print(new_data.head())\n",
    "print('Total rows:', len(new_data))\n",
    "print('Total rows:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c69c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d804c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
