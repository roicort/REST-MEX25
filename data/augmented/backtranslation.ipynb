{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e29964e",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b0bd0",
   "metadata": {},
   "source": [
    "### Backtranslation with facebook/nllb-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fda171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2a0bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando MPS: mps\n",
      "Tensor de prueba creado en el dispositivo: tensor([1.], device='mps:0') mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from utils.config import setConfig\n",
    "\n",
    "device = setConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f918e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtranslation \n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model_name1 = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(model_name1)\n",
    "\n",
    "model_name2 = \"Helsinki-NLP/opus-mt-fr-es\"\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(model_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd256091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translatorES2EN = pipeline('translation', model=model1, tokenizer=tokenizer1, src_lang='es_Latn', tgt_lang='fra_Latn', max_length=512)\n",
    "\n",
    "translatorEN2ES = pipeline('translation', model=model2, tokenizer=tokenizer2, src_lang='fra', tgt_lang='spa', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678a2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(text, trans1, trans2):\n",
    "    translated_text = trans1(text)[0]['translation_text']\n",
    "    backtranslated = trans2(translated_text)[0]['translation_text']\n",
    "    return backtranslated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f80628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'../../data/train/train.csv')\n",
    "\n",
    "data['Title'] = data['Title'].astype(str)\n",
    "data['Review'] = data['Review'].astype(str)\n",
    "data['Town'] = data['Town'].astype(str)\n",
    "data['Region'] = data['Region'].astype(str)\n",
    "data['Type'] = data['Type'].astype(str)\n",
    "data['Polarity'] = data['Polarity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37ad01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews1: 32646\n",
      "Reviews2: 32976\n",
      "Reviews3: 93114\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "\n",
    "Reviews1 = data[data['Polarity'] == 1]\n",
    "Reviews2 = data[data['Polarity'] == 2]\n",
    "Reviews3 = data[data['Polarity'] == 3]\n",
    "\n",
    "print('Reviews1:', Reviews1.size)\n",
    "print('Reviews2:', Reviews2.size)\n",
    "print('Reviews3:', Reviews3.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows1 = []\n",
    "for i in tqdm(random.sample(range(0, len(Reviews1)), int(len(Reviews1) * 0.5))):\n",
    "    new_row = {\n",
    "        'Title':  back_translate(Reviews1['Title'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Review': back_translate(Reviews1['Review'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Town': Reviews1['Town'].iloc[i],\n",
    "        'Region': Reviews1['Region'].iloc[i],\n",
    "        'Type': Reviews1['Type'].iloc[i],\n",
    "        'Polarity': Reviews1['Polarity'].iloc[i]\n",
    "    }\n",
    "\n",
    "    new_rows1.append(new_row)\n",
    "\n",
    "new_rows2 = []\n",
    "for i in tqdm(random.sample(range(0, len(Reviews2)), int(len(Reviews2) * 0.5))):\n",
    "    new_row = {\n",
    "        'Title':  back_translate(Reviews2['Title'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Review': back_translate(Reviews2['Review'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Town': Reviews2['Town'].iloc[i],\n",
    "        'Region': Reviews2['Region'].iloc[i],\n",
    "        'Type': Reviews2['Type'].iloc[i],\n",
    "        'Polarity': Reviews2['Polarity'].iloc[i]\n",
    "    }\n",
    "\n",
    "    new_rows2.append(new_row)\n",
    "\n",
    "new_rows3 = []\n",
    "for i in tqdm(random.sample(range(0, len(Reviews3)), int(len(Reviews3) * 0.5))):\n",
    "    new_row = {\n",
    "        'Title':  back_translate(Reviews3['Title'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Review': back_translate(Reviews3['Review'].iloc[i][:512], translatorES2EN, translatorEN2ES),\n",
    "        'Town': Reviews3['Town'].iloc[i],\n",
    "        'Region': Reviews3['Region'].iloc[i],\n",
    "        'Type': Reviews3['Type'].iloc[i],\n",
    "        'Polarity': Reviews3['Polarity'].iloc[i]\n",
    "    }\n",
    "\n",
    "    new_rows3.append(new_row)\n",
    "\n",
    "# Crear un nuevo DataFrame con las nuevas filas\n",
    "new_data1 = pd.DataFrame(new_rows1)\n",
    "new_data2 = pd.DataFrame(new_rows2)\n",
    "new_data3 = pd.DataFrame(new_rows3)\n",
    "# Concatenar los DataFrames\n",
    "new_data = pd.concat([new_data1, new_data2, new_data3], ignore_index=True)\n",
    "# Guardar el nuevo DataFrame en un archivo CSV\n",
    "new_data.to_csv(r'../../data/augmented/train_backtranslated.csv', index=False)\n",
    "\n",
    "# Imprimir el nuevo DataFrame\n",
    "print(new_data.head())\n",
    "print('Total rows:', len(new_data))\n",
    "print('Total rows:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c69c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d804c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
