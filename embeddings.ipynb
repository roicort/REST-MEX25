{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/NLP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import wasabi\n",
    "import transformers\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() \n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "from metrics import RestMexMetrics\n",
    "metrics = RestMexMetrics()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'/Users/roicort/GitHub/REST-MEX25/dataset/train.csv')\n",
    "\n",
    "data['Title'] = data['Title'].astype(str)\n",
    "data['Review'] = data['Review'].astype(str)\n",
    "data['Town'] = data['Town'].astype(str)\n",
    "data['Region'] = data['Region'].astype(str)\n",
    "data['Type'] = data['Type'].astype(str)\n",
    "data['Polarity'] = data['Polarity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (176843, 6)\n",
      "Test shape: (31208, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train, test = train_test_split(data, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "X_train = '<title>' + train['Title'] + '<title> <review>' + train['Review'] + '<review>'\n",
    "X_test = '<title>' + test['Title'] + '<title> <review>' + test['Review'] + '<review>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"CarlosRCDev/spanish-gte-multilingual-base\"\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedd \n",
    "\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "X_train_embeddings = []\n",
    "X_test_embeddings = []\n",
    "\n",
    "for text in tqdm(X_train):\n",
    "    embedding = embed_text(text)\n",
    "    X_train_embeddings.append(embedding)\n",
    "\n",
    "for text in tqdm(X_test):\n",
    "    embedding = embed_text(text)\n",
    "    X_test_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings\n",
    "joblib.dump(X_train_embeddings, 'X_train_embeddings.pkl')\n",
    "joblib.dump(X_test_embeddings, 'X_test_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = joblib.load('X_train_embeddings.pkl')\n",
    "X_test_embeddings = joblib.load('X_test_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model\n",
      "Accuracy: 0.7434632145603691\n",
      "F1 Score: 0.7223601392307758\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.61      0.61       794\n",
      "           2       0.40      0.28      0.33       829\n",
      "           3       0.55      0.48      0.51      2318\n",
      "           4       0.54      0.35      0.43      6935\n",
      "           5       0.81      0.93      0.87     20332\n",
      "\n",
      "    accuracy                           0.74     31208\n",
      "   macro avg       0.58      0.53      0.55     31208\n",
      "weighted avg       0.72      0.74      0.72     31208\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/NLP/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5498756818619603"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "y_train = train['Polarity']\n",
    "y_test = test['Polarity']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_embeddings, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test_embeddings)\n",
    "print(\"Logistic Regression Model\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=test['Polarity'].astype(int).unique(), output_dict=True)\n",
    "report = pd.DataFrame(report)\n",
    "f1 = report[y_test.unique()].loc['f1-score'].to_dict()\n",
    "\n",
    "ResP_k = metrics.TypeScore(f1)\n",
    "\n",
    "ResP_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_embeddings_reduced = pca.fit_transform(X_train_embeddings)\n",
    "# Create a DataFrame for the reduced embeddings\n",
    "df_embeddings_train = pd.DataFrame(X_train_embeddings_reduced, columns=['x', 'y'])\n",
    "df_embeddings_train['Polarity'] = y_train.values\n",
    "# Plot the reduced embeddings\n",
    "fig = px.scatter(df_embeddings_train, x='x', y='y', color='Polarity', title=\"PCA of Embeddings\")\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title=\"PCA of Train Embeddings\", xaxis_title=\"PCA 1\", yaxis_title=\"PCA 2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model\n",
      "Accuracy: 0.9591771340681876\n",
      "F1 Score: 0.9591545517717532\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Attractive       0.96      0.96      0.96     10472\n",
      "       Hotel       0.95      0.94      0.95      7729\n",
      "  Restaurant       0.96      0.97      0.96     13007\n",
      "\n",
      "    accuracy                           0.96     31208\n",
      "   macro avg       0.96      0.96      0.96     31208\n",
      "weighted avg       0.96      0.96      0.96     31208\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9576700132343521"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "y_train = train['Type']\n",
    "y_test = test['Type']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_embeddings, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test_embeddings)\n",
    "print(\"Logistic Regression Model\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=test['Type'].unique(), output_dict=True)\n",
    "report = pd.DataFrame(report)\n",
    "f1 = report[['Attractive', 'Hotel', 'Restaurant']].loc['f1-score'].to_dict()\n",
    "\n",
    "ResT_k = metrics.TypeScore(f1)\n",
    "ResT_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model\n",
      "Accuracy: 0.6066713663163291\n",
      "F1 Score: 0.6275209800292063\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                    Ajijic       0.63      0.51      0.56       566\n",
      "                   Atlixco       0.36      0.67      0.47       207\n",
      "                   Bacalar       0.23      0.72      0.34      1657\n",
      "                    Bernal       0.44      0.73      0.55       177\n",
      "           Chiapa_de_Corzo       0.12      0.65      0.21       143\n",
      "                   Cholula       0.74      0.50      0.60       425\n",
      "                  Coatepec       0.41      0.74      0.53        98\n",
      "                     Creel       1.00      1.00      1.00       274\n",
      "           Cuatro_Cienegas       0.77      0.74      0.75       133\n",
      "                 Cuetzalan       0.31      0.31      0.31       153\n",
      "           Dolores_Hidalgo       1.00      1.00      1.00       126\n",
      "          Huasca_de_Ocampo       1.00      1.00      1.00       247\n",
      "              Isla_Mujeres       0.60      0.49      0.54      4447\n",
      "         Ixtapan_de_la_Sal       0.29      0.43      0.35       251\n",
      "                    Izamal       0.32      0.64      0.42       332\n",
      "                    Loreto       0.76      0.64      0.69       778\n",
      "                 Malinalco       0.15      0.24      0.18       215\n",
      "                   Mazunte       1.00      1.00      1.00       210\n",
      "                   Metepec       0.44      0.71      0.54       555\n",
      "                   Orizaba       0.92      0.72      0.80       379\n",
      "                  Palenque       0.58      0.40      0.47      1430\n",
      "                    Parras       0.78      0.81      0.79       153\n",
      "                 Patzcuaro       1.00      1.00      1.00       615\n",
      "           Real_de_Catorce       0.61      0.77      0.68       126\n",
      "San_Cristobal_de_las_Casas       0.72      0.65      0.68      1902\n",
      "                  Sayulita       1.00      1.00      1.00      1143\n",
      "                   Tapalpa       0.18      0.39      0.24       100\n",
      "                     Taxco       1.00      1.00      1.00       659\n",
      "               Teotihuacan       0.80      0.68      0.74       879\n",
      "               Tepotzotlan       0.16      0.32      0.21       173\n",
      "                 Tepoztlan       1.00      1.00      1.00       527\n",
      "                   Tequila       0.45      0.70      0.55       392\n",
      "             Tequisquiapan       0.89      0.69      0.78       543\n",
      "               Tlaquepaque       0.64      0.40      0.49       588\n",
      "               TodosSantos       0.66      0.78      0.71       696\n",
      "                     Tulum       0.80      0.48      0.60      6860\n",
      "                Valladolid       0.91      0.73      0.81      1705\n",
      "            Valle_de_Bravo       0.74      0.21      0.32       882\n",
      "                   Xilitla       0.84      0.71      0.77       209\n",
      "                  Zacatlan       0.58      0.47      0.52       253\n",
      "\n",
      "                  accuracy                           0.61     31208\n",
      "                 macro avg       0.65      0.67      0.63     31208\n",
      "              weighted avg       0.70      0.61      0.63     31208\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6307106321888962"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train['Town']\n",
    "y_test = test['Town']\n",
    "\n",
    "# Utilizamos los embeddings y la columna de region para predecir, es decir tenemos que crear un nuevo dataframe \n",
    "# Donde una columna sea la region y la otra el embedding\n",
    "X_train_mt = pd.DataFrame(X_train_embeddings)\n",
    "X_train_mt['Region'] = train['Region'].values\n",
    "# Convertir a categorical\n",
    "X_train_mt['Region'] = X_train_mt['Region'].astype('category')\n",
    "X_train_mt['Region'] = X_train_mt['Region'].cat.codes\n",
    "X_train_mt.columns = X_train_mt.columns.astype(str)\n",
    "\n",
    "X_test_embeddings = pd.DataFrame(X_test_embeddings)\n",
    "X_test_embeddings['Region'] = test['Region'].values\n",
    "# Convertir a categorical\n",
    "X_test_embeddings['Region'] = X_test_embeddings['Region'].astype('category')\n",
    "X_test_embeddings['Region'] = X_test_embeddings['Region'].cat.codes\n",
    "X_test_embeddings.columns = X_test_embeddings.columns.astype(str)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "lr = GaussianNB()\n",
    "lr.fit(X_train_mt, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test_embeddings)\n",
    "print(\"Logistic Regression Model\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=test['Town'].unique(), output_dict=True)\n",
    "report = pd.DataFrame(report)\n",
    "f1 = report[y_test.unique()].loc['f1-score'].to_dict()\n",
    "\n",
    "ResMT_k = metrics.TypeScore(f1)\n",
    "\n",
    "ResMT_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
